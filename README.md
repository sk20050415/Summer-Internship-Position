# Vision-based robotic arm trajectory planning and intelligent grasping
## Contents
- [Install](#install)
- [Simulation](#simulation)
- [Trajectory Planning](#trajectory-planning)
- [Intelligent Grasping](#intelligent-grasping)

## Install

**System Requirements**
- ubuntu22.04.3 LTS
- CUDA11.7
- Pytorch1.13.0
- python3.10

**Library Installation**
- [cuRobo](https://curobo.org/get_started/1_install_instructions.html)
- [Flexiv RDK](https://github.com/flexivrobotics/flexiv_rdk/tree/main)

*The version of cuRobo applied is v0.6.2, and the version of Flexiv RDK is v0.9.*

**Using in Isaac Sim**
- [Isaac Sim 2022.2.1](https://docs.omniverse.nvidia.com/isaacsim/latest/installation/install_workstation.html)

*:grey_exclamation:[System requirements](https://docs.omniverse.nvidia.com/isaacsim/latest/installation/requirements.html).*

**Installation for intelligent grasping**
- [ZED SDK](https://www.stereolabs.com/docs/app-development/python/install)
- OpenAI (pip)

## Simulation

In the project, the simulation scene is constructed in Isaac Sim, with features of components consistent with those in the real working
environment. The paths of grasping and placing tubes is created by the cuRobo trajectory planning algorithm. As to using cuRobo in Isaac Sim and applying for a new robot, read the instruction below.  
- [cuRobo instruction](https://curobo.org/get_started/2b_isaacsim_examples.html)  
- [Configuring a new robot](https://curobo.org/tutorials/1_robot_configuration.html)
- [Other possible detailed problems on Isaac Sim](https://forums.developer.nvidia.com/c/omniverse/simulation/69)
- [Other possible detailed problems on cuRobo](https://github.com/NVlabs/curobo/discussions)

[Example code](https://github.com/Follograph/TPIG/tree/main/trajectory%20planning/simulation)  
Explanation:  
In **single_complete.py**, a flexiv robot picks a cube to another place.  
In **novisual.py**, the robot with extended finger tips picks the tube from the box to another box, and **withplot.py** records changes in joints during one of the motion.  
If you want to run the script, turn to the folder:  
```
omni_python novisual.py
```

The [config](https://github.com/Follograph/TPIG/tree/main/trajectory%20planning/config) includes the configuration of the flexiv rizon4 robot and yaml files for the robot and the simulation environment. If you need to use yaml files for cuRobo, please allocate them in the right file folder.

## Trajectory-Planning

The robotic arm is remotely controlled with the help of flexiv RDK library. Some [examples](https://github.com/Follograph/TPIG/tree/main/trajectory%20planning) are given to try different controlling mode.  
*:grey_exclamation: Change robot_ip and local_ip to your own one. Change the RDK package path to your own one.*  
- [Normal tube picking mode](https://github.com/Follograph/TPIG/tree/main/trajectory%20planning/normal)  
*In these codes, mulitiassedit is the complete final version(implements complete pick-up, movement, and placement and involves the camera part), and the other three codes are the testing versions(more details in the annotations in these files )*  
In the normal tube picking mode, the robotic arm is controlled in a three-coordinate manner, with motion generated by the MoveL primitive. In **simpleassedit.py**, one tube is picked and placed while the robotic arm can pick and place a row of tubes in **multiassedit.py**. **Multitestedit.py** and **multitestendedit.py** are separately used to check the accuracy of picking and placing positions.  
- [Anomaly handling mode (leaning, lying)](https://github.com/Follograph/TPIG/tree/main/trajectory%20planning/abnormal)   
*In the "Abnormal" folder, we provide the codes to cope with the leaning and lying states and the utility functions* 

In the anomaly handling mode, the robotic arm is controlled in the six-degree-of-freedom manner, with part of the trajectory generated by cuRobo, motion generated by joint position mode and MoveL primitive. **Motion_incline_two.py** is for picking the leaning tube while **motion_lie.py** is for picking the lying one. Additionally, in motion_incline_two.py the trajectory points of the end effector are written into a CSV file.

## Intelligent-Grasping

Use GPT-4o to recognize images of grasping scenarios. Images are captured by ZED2 camera, which is not recommended.  
As to the use of API, read the [gpt cookbook](https://platform.openai.com/docs/guides/vision).   
The paper [GPT4Vision-Robot-Manipulation-Prompts](https://github.com/microsoft/GPT4Vision-Robot-Manipulation-Prompts?tab=readme-ov-file) can be a reference for prompts.

1. Install openAI and ZED SDK.
2. Run **comtest.py** to recognize the image and apply correct robot actions (**robot_actions.py**). There are six types of grasping scenarios in total.

# 3D visual image segmentation and pose estimation in grasping applications
please read the [Adventure](https://github.com/AdventurerDXC/SJTUME24-gradproject).  
During my reimplementation of this part, the "nanosam" part have been successfully reimplement. To reimplement this part, you can enter the Terminal of the computer and then input "conda activate base" to enter the virtual environment, and then you can try the examples according to [nanosam](https://github.com/NVIDIA-AI-IOT/nanosam).  
Regarding the "foundationpose" part, you can follow the ["Env setup option1 docker"](https://github.com/NVlabs/FoundationPose) to try finishing the setup of the environment also in the virtual environment named "base". However, it seems that some trouble exists in this computer's nvidia settings. You can input "bash build_all.sh" to read the error. 
